---
title: 'Inference Arithmetic'
description: |
  Arithmetic on LLM inference performance
pubDate: 'Oct 22 2025'
slug: inference-arithmetic
index: false
showConfigSidebar: true
stickyToc: false
---

import Sidenote from '../../components/Sidenote.astro'
import ConfigTable from '../../components/inference/ConfigTable.tsx'
import GPUSpecsTable from '../../components/inference/GPUSpecsTable.tsx'
import ModelParamsTable from '../../components/inference/ModelParamsTable.tsx'
import ComputeBoundThreshold from '../../components/inference/ComputeBoundThreshold.tsx'
import PrefillCalculation from '../../components/inference/PrefillCalculation.tsx'
import DecodeCalculation from '../../components/inference/DecodeCalculation.tsx'
import LatencyCalculation from '../../components/inference/LatencyCalculation.tsx'
import ThroughputCalculation from '../../components/inference/ThroughputCalculation.tsx'
import ChunkedPrefillingCalculation from '../../components/inference/ChunkedPrefillingCalculation.tsx'
import NonOverlappedPrefillTokens from '../../components/inference/NonOverlappedPrefillTokens.tsx'
import BenchmarkComparison from '../../components/inference/BenchmarkComparison.tsx'
import ThroughputChart from '../../components/inference/ThroughputChart.tsx'
import LatencyChart from '../../components/inference/LatencyChart.tsx'

How fast can an LLM go? <Sidenote> Great resources for doing this kind of of thinking: [here](https://kipp.ly/transformer-inference-arithmetic/), [here](https://jax-ml.github.io/scaling-book/)</Sidenote>

Reading this [article](https://inferencemax.semianalysis.com/) made me curious as to how good our software is actually getting.
Lets make some assumptions, and then work it through.

Pick a configuration from the selector on the left to change the values throughout this article.

<div class="xl:hidden">
**Configuration**
<ConfigTable client:load />

</div>

<Sidenote unnumbered>
 Why do NVIDIA still do TFLOPs with sparsity? Is someone using this? Is
 everyone just going to be dividing their numbers by 2 from now until forever?
</Sidenote>

<GPUSpecsTable client:load />

## Working it through

We need to start counting operations and memory.

Transformers have a lot of different operations in them: softmax, RMSNorm,
matrix multiplications, biases, attention, SiLU, etc. etc. But the good news for doing
stuff with rules of thumb is that almost none of them matter. Matrix
multiplications $(F, D) \times (D, B)$ take a number of FLOPS $O(2 BDF)$, and
require us to transfer $O(n_{\mathrm{bytes}}(BD + DF + BF))$<Sidenote>$n_{\mathrm{bytes}}$ is the number of bytes per parameter - i.e. $1$ for fp8, $2$ for bf16, ..</Sidenote> values from HBM to & from the
compute units. This scaling dominates everything else (softmaxes, norms, etc.
are all linear).

Since almost all the models the parameters are in the matmuls, and the number
of FLOPS for an individual matmul is $2 \times \# \mathrm{parameters}$, then we
can get the total FLOPS for a forward pass through the transformer from just $2
\cdot P$, where $P$ is the total number of parameters. Read the article
[here](https://kipp.ly/transformer-inference-arithmetic/#flops-counting)<Sidenote>The
calculation in that article is for various 2022 era transformers, but LLama-3
isn't all that different. The only meaningful change is GQA, but that doesn't
change the formula</Sidenote> to see this worked out.

There are two meaningful bottlenecks that well-optimized GPU software should
hit: we can be **memory bandwidth bound**, or we can be **compute bound**. GPUs
at the highest level do just two things - transfer data to compute units, and
do computation on that data. Since the two can happen at the same time, we only
need to care about the longer of the two. Which one we hit depends on our
workload.

<Sidenote unnumbered>
 Note: $B$ here is doing double duty in this article - below its the batch size
 passing through the transformer (i.e. number of sequences), here its the
 number of tokens passing through the matmul. Also, the approximation comes
 from $D \gg B$.
</Sidenote>

Which bucket does transformer inference fall into? Well, it depends. For a
single $(F, D) \times (D, B)$ matmul to be compute bound, we need the
arithmetic intensity to exceed the accelerator's compute to bandwidth ratio:

$$
\frac{2BDF}{n_\mathrm{bytes}(BD + DF + BF)} \approx \frac{2B}{n_\mathrm{bytes}} \geq \frac{\text{accelerator compute}}{\text{accelerator bandwidth}}
$$

So, we get our first result, the threshold number of tokens at which
transformer inference becomes completely compute bound:

<Sidenote unnumbered>
 Mouse over the underlined numbers to see their source
</Sidenote>

<ComputeBoundThreshold client:load />

Try changing the accelerator to see how this threshold changes.

### Prefill

Transformer inference has two phases: prefill, where we process the input
prompt, and then decode, where we generate output tokens one at a time.
Prefilling happens in parallel across all the input tokens. For all the
configurations (change the sequence length selector and see), prefilling is
compute bound, even without batching across multiple requests.

Compute bound means that we're bounded by the FLOP/s of the accelerator: i.e.
its calculating just as hard as it can.

We worked out the FLOPS in a transformer forward pass above: its $2BP$, for a
model of size $P$, and batch size $B$. So, given the FLOP/s of the accelerator,
we can figure how long prefill should take for a single sequence: <Sidenote> Since we're compute bound, computing for $N$ sequences just means multiplying this number by $N$.</Sidenote>

<PrefillCalculation client:load />

### Decode

What about decode? Since decode deals with 1 token at a time per sequence, if
the batch size is below the compute bound threshold, we will be memory
bandwidth bound.

Bandwidth bound means our compute units are starved of work, and we're spending
more time transferring data than computing on it. If we're memory bandwidth
bound, then, instead of comparing FLOPs, we need to compare the memory
transfers to the accelerator memory bandwidth.

There are two things we need to transfer for each decode step: the weights and
the KV cache. For each token, we store both a key and value vector for each
layer. The sequences run from ISL to ISL+OSL tokens, so we'll assume that the
average sequence length is in the middle.

<DecodeCalculation client:load />

<Sidenote unnumbered>
 Note the memory bandwidth is multiplied by the tensor parallelism factor
</Sidenote>

## Results

### End to End latency

The fastest end-to-end latency that's possible for any single request is the prefill time plus
the decode time multiplied by the number of output tokens:

<LatencyCalculation client:load />

### Throughput

To calculate throughput, we need to account for both prefill and decode phases.
The calculation depends on how we schedule these phases.
If we do what was done in popular inference engines in the last few years, we
run decode until we get new sequences to prefill, and then pause to prefill
them.

This isn't broadly the case any more. We can do 'chunked prefilling' instead,
where we have 'heterogeneous' batches, that contain both parts of a prefill,
alongside a number of ongoing decodes.

This means in principle we can use some of the idle compute during decodes to prefill new
sequences.

<ThroughputCalculation client:load />

### Comparison with Benchmarks

<BenchmarkComparison client:load />

<ThroughputChart client:load />

<LatencyChart client:load />

---

## Appendix: Chunked Prefilling

vLLM's default scheduler (as of v0.10.0) uses **chunked prefilling**. The idea
is that prefills and decodes have complementary resource usage: prefills are
compute bound (wasting memory bandwidth), while decodes are memory bandwidth
bound (wasting compute). By mixing them in the same batch, we can increase
utilization. To do this, we need to write kernels that can handle both at the
same time, but kindly somebody did
[it](https://github.com/flashinfer-ai/flashinfer).

The scheduler works as follows: at each step, we have a token budget
(`max_num_batched_tokens`). Ongoing decodes take priority, using 1 token per
sequence. Any remaining budget is filled with prefill tokens from incoming
requests. If a prefill is too large to fit entirely, it gets split into chunks
that are processed across multiple steps.

In steady state, we have all the concurrent users continuously decoding. When a
sequence completes, a new request will arrive to take its place. That new
request will be prefilled (potentially in chunks) alongside the ongoing decodes.

In order not to interrupt decodes, we require the chunk size to be equal to:

$$
\mathrm{Chunk~size} = \mathrm{Compute~bound~threshold}-B
$$

The condition here is that decode batches are made perfectly compute bound by the
addition of prefill chunks.

If this is the case, then we can do $OSL$ prefill chunks 'for free' in parallel
with the decodes. There are:

$$
B \times (ISL/\mathrm{Chunk~size})
$$

chunks to process in total. So the number of non-overlapped prefill _tokens_ is:

$$
\begin{aligned}
(B \times (ISL/\mathrm{Chunk~size}) - OSL) \times \mathrm{Chunk~size} \\
B \times ISL - OSL \times \mathrm{Chunk~size} \\
B \times (ISL + OSL) - OSL \times \mathrm{Compute~bound~threshold}
\end{aligned}
$$

For the currently selected configuration:

<NonOverlappedPrefillTokens client:load />
